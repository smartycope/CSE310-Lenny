{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-prIxCiGJxlm"
      },
      "source": [
        "# Deep Deterministic Policy Gradient (DDPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtbqSHEVJxlq"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "**Deep Deterministic Policy Gradient (DDPG)** is a model-free off-policy algorithm for\n",
        "learning continous actions.\n",
        "\n",
        "It combines ideas from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network).\n",
        "It uses Experience Replay and slow-learning target networks from DQN, and it is based on\n",
        "DPG,\n",
        "which can operate over continuous action spaces.\n",
        "\n",
        "This tutorial closely follow this paper -\n",
        "[Continuous control with deep reinforcement learning](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "\n",
        "## Problem\n",
        "\n",
        "We are trying to solve the classic **Inverted Pendulum** control problem.\n",
        "In this setting, we can take only two actions: swing left or swing right.\n",
        "\n",
        "What make this problem challenging for Q-Learning Algorithms is that actions\n",
        "are **continuous** instead of being **discrete**. That is, instead of using two\n",
        "discrete actions like `-1` or `+1`, we have to select from infinite actions\n",
        "ranging from `-2` to `+2`.\n",
        "\n",
        "## Quick theory\n",
        "\n",
        "Just like the Actor-Critic method, we have two networks:\n",
        "\n",
        "1. Actor - It proposes an action given a state.\n",
        "2. Critic - It predicts if the action is good (positive value) or bad (negative value)\n",
        "given a state and an action.\n",
        "\n",
        "DDPG uses two more techniques not present in the original DQN:\n",
        "\n",
        "**First, it uses two Target networks.**\n",
        "\n",
        "**Why?** Because it add stability to training. In short, we are learning from estimated\n",
        "targets and Target networks are updated slowly, hence keeping our estimated targets\n",
        "stable.\n",
        "\n",
        "Conceptually, this is like saying, \"I have an idea of how to play this well,\n",
        "I'm going to try it out for a bit until I find something better\",\n",
        "as opposed to saying \"I'm going to re-learn how to play this entire game after every\n",
        "move\".\n",
        "See this [StackOverflow answer](https://stackoverflow.com/a/54238556/13475679).\n",
        "\n",
        "**Second, it uses Experience Replay.**\n",
        "\n",
        "We store list of tuples `(state, action, reward, next_state)`, and instead of\n",
        "learning only from recent experience, we learn from sampling all of our experience\n",
        "accumulated so far.\n",
        "\n",
        "Now, let's see how is it implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fiSlHr6hJxlr"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from cartpole import CartPoleEnv\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXkhVHnkJxls"
      },
      "source": [
        "We use [OpenAIGym](http://gym.openai.com/docs) to create the environment.\n",
        "We will use the `upper_bound` parameter to scale our actions later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RmLtxQ4_Jxlt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of State Space ->  3\n",
            "Size of Action Space ->  1\n",
            "Max Value of Action ->  2.0\n",
            "Min Value of Action ->  -2.0\n"
          ]
        }
      ],
      "source": [
        "problem = \"Pendulum-v1\"\n",
        "# env = gym.make(problem, 1000, render_mode='human')\n",
        "env = gym.make(problem, 300)\n",
        "# env = CartPoleEnv(render_mode='pygame')\n",
        "\n",
        "num_states = env.observation_space.shape[0]\n",
        "print(\"Size of State Space ->  {}\".format(num_states))\n",
        "num_actions = env.action_space.shape[0]\n",
        "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
        "\n",
        "upper_bound = env.action_space.high[0]\n",
        "lower_bound = env.action_space.low[0]\n",
        "\n",
        "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
        "print(\"Min Value of Action ->  {}\".format(lower_bound))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdsVit7fJxlu"
      },
      "source": [
        "The `Buffer` class implements Experience Replay.\n",
        "\n",
        "---\n",
        "![Algorithm](https://i.imgur.com/mS6iGyJ.jpg)\n",
        "---\n",
        "\n",
        "\n",
        "**Critic loss** - Mean Squared Error of `y - Q(s, a)`\n",
        "where `y` is the expected return as seen by the Target network,\n",
        "and `Q(s, a)` is action value predicted by the Critic network. `y` is a moving target\n",
        "that the critic model tries to achieve; we make this target\n",
        "stable by updating the Target model slowly.\n",
        "\n",
        "**Actor loss** - This is computed using the mean of the value given by the Critic network\n",
        "for the actions taken by the Actor network. We seek to maximize this quantity.\n",
        "\n",
        "Hence we update the Actor network so that it produces actions that get\n",
        "the maximum predicted value as seen by the Critic, for a given state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-P0XRgsCJxlv"
      },
      "outputs": [],
      "source": [
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tf.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
        "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alNdizGxJxlv"
      },
      "source": [
        "Here we define the Actor and Critic networks. These are basic Dense models\n",
        "with `ReLU` activation.\n",
        "\n",
        "Note: We need the initialization for last layer of the Actor to be between\n",
        "`-0.003` and `0.003` as this prevents us from getting `1` or `-1` output values in\n",
        "the initial stages, which would squash our gradients to zero,\n",
        "as we use the `tanh` activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JcmJv9B7Jxlw"
      },
      "outputs": [],
      "source": [
        "# Actor & Critic\n",
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tf.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9OZchV4Jxlw"
      },
      "source": [
        "`policy()` returns an action sampled from our Actor network plus some noise for\n",
        "exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P7PeV6E2Jxlw"
      },
      "outputs": [],
      "source": [
        "def policy(state, noise_func):\n",
        "    sampled_actions = tf.squeeze(actor_model(state))\n",
        "\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise_func()\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84aR4JjSJxlw"
      },
      "source": [
        "## Training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "A25VQyAEJxlx"
      },
      "outputs": [],
      "source": [
        "# Running this cell resets the weights\n",
        "noise = lambda mu=0, std_dev=0.1: random.gauss(mu, std_dev)\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 100\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "# tau = 0.0001\n",
        "\n",
        "buffer = Buffer(50000, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILFm3xXLJxlx"
      },
      "source": [
        "Now we implement our main training loop, and iterate over episodes.\n",
        "We sample actions using `policy()` and train with `learn()` at each time step,\n",
        "along with updating the Target networks at a rate `tau`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8Stxw_dQJxlx"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/anastasia/hello/python/CSE310-Lenny/agent copy.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Don't start slowing down until we've gotten past the first bit\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# if len(ep_reward_list) < 0 or len(ep_reward_list) > 75:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# Show the enviorment as it's updated\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# env.render()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     tf_prev_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(tf\u001b[39m.\u001b[39mconvert_to_tensor(prev_state), \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     action \u001b[39m=\u001b[39m policy(tf_prev_state, noise)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# print(round(action[0]), 2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# Recieve state and reward from environment.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     state, reward, done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
            "\u001b[1;32m/home/anastasia/hello/python/CSE310-Lenny/agent copy.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpolicy\u001b[39m(state, noise_func):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     sampled_actions \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msqueeze(actor_model(state))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# Adding noise to action\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/anastasia/hello/python/CSE310-Lenny/agent%20copy.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     sampled_actions \u001b[39m=\u001b[39m sampled_actions\u001b[39m.\u001b[39mnumpy() \u001b[39m+\u001b[39m noise_func()\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/training.py:590\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    588\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 590\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/base_layer.py:1127\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1124\u001b[0m     name_scope \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_unnested_name_scope()\n\u001b[1;32m   1125\u001b[0m     call_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autographed_call()\n\u001b[0;32m-> 1127\u001b[0m call_fn \u001b[39m=\u001b[39m traceback_utils\u001b[39m.\u001b[39;49minject_argument_info_in_traceback(\n\u001b[1;32m   1128\u001b[0m     call_fn,\n\u001b[1;32m   1129\u001b[0m     object_name\u001b[39m=\u001b[39;49m(\n\u001b[1;32m   1130\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlayer \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m (type \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m   1131\u001b[0m     ),\n\u001b[1;32m   1132\u001b[0m )\n\u001b[1;32m   1133\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mExitStack() \u001b[39mas\u001b[39;00m namescope_stack:\n\u001b[1;32m   1134\u001b[0m     \u001b[39mif\u001b[39;00m _is_name_scope_on_model_declaration_enabled:\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:160\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback\u001b[0;34m(fn, object_name)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mdel\u001b[39;00m signature\n\u001b[1;32m    158\u001b[0m         \u001b[39mdel\u001b[39;00m bound_signature\n\u001b[0;32m--> 160\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49mdecorator\u001b[39m.\u001b[39;49mmake_decorator(fn, error_handler)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/util/tf_decorator.py:136\u001b[0m, in \u001b[0;36mmake_decorator\u001b[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m decorator_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m   decorator_name \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mcurrentframe()\u001b[39m.\u001b[39mf_back\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\n\u001b[0;32m--> 136\u001b[0m decorator \u001b[39m=\u001b[39m TFDecorator(decorator_name, target, decorator_doc,\n\u001b[1;32m    137\u001b[0m                         decorator_argspec)\n\u001b[1;32m    138\u001b[0m \u001b[39msetattr\u001b[39m(decorator_func, \u001b[39m'\u001b[39m\u001b[39m_tf_decorator\u001b[39m\u001b[39m'\u001b[39m, decorator)\n\u001b[1;32m    139\u001b[0m \u001b[39m# Objects that are callables (e.g., a functools.partial object) may not have\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39m# the following attributes.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/util/tf_decorator.py:332\u001b[0m, in \u001b[0;36mTFDecorator.__init__\u001b[0;34m(self, decorator_name, target, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mcallable\u001b[39m(target):\n\u001b[1;32m    331\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__signature__ \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49msignature(target)\n\u001b[1;32m    333\u001b[0m   \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m     \u001b[39m# Certain callables such as builtins can not be inspected for signature.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib64/python3.11/inspect.py:3280\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msignature\u001b[39m(obj, \u001b[39m*\u001b[39m, follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39mglobals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_str\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   3279\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3280\u001b[0m     \u001b[39mreturn\u001b[39;00m Signature\u001b[39m.\u001b[39;49mfrom_callable(obj, follow_wrapped\u001b[39m=\u001b[39;49mfollow_wrapped,\n\u001b[1;32m   3281\u001b[0m                                    \u001b[39mglobals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mglobals\u001b[39;49m, \u001b[39mlocals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mlocals\u001b[39;49m, eval_str\u001b[39m=\u001b[39;49meval_str)\n",
            "File \u001b[0;32m/usr/lib64/python3.11/inspect.py:3028\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   3025\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_callable\u001b[39m(\u001b[39mcls\u001b[39m, obj, \u001b[39m*\u001b[39m,\n\u001b[1;32m   3026\u001b[0m                   follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39mglobals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_str\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   3027\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3028\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   3029\u001b[0m                                     follow_wrapper_chains\u001b[39m=\u001b[39;49mfollow_wrapped,\n\u001b[1;32m   3030\u001b[0m                                     \u001b[39mglobals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mglobals\u001b[39;49m, \u001b[39mlocals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mlocals\u001b[39;49m, eval_str\u001b[39m=\u001b[39;49meval_str)\n",
            "File \u001b[0;32m/usr/lib64/python3.11/inspect.py:2454\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2449\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m is not a callable object\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(obj))\n\u001b[1;32m   2451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, types\u001b[39m.\u001b[39mMethodType):\n\u001b[1;32m   2452\u001b[0m     \u001b[39m# In this case we skip the first parameter of the underlying\u001b[39;00m\n\u001b[1;32m   2453\u001b[0m     \u001b[39m# function (usually `self` or `cls`).\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m     sig \u001b[39m=\u001b[39m _get_signature_of(obj\u001b[39m.\u001b[39;49m\u001b[39m__func__\u001b[39;49m)\n\u001b[1;32m   2456\u001b[0m     \u001b[39mif\u001b[39;00m skip_bound_arg:\n\u001b[1;32m   2457\u001b[0m         \u001b[39mreturn\u001b[39;00m _signature_bound_method(sig)\n",
            "File \u001b[0;32m/usr/lib64/python3.11/inspect.py:2516\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2511\u001b[0m             \u001b[39mreturn\u001b[39;00m sig\u001b[39m.\u001b[39mreplace(parameters\u001b[39m=\u001b[39mnew_params)\n\u001b[1;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m isfunction(obj) \u001b[39mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2514\u001b[0m     \u001b[39m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2515\u001b[0m     \u001b[39m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[0;32m-> 2516\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[1;32m   2517\u001b[0m                                     skip_bound_arg\u001b[39m=\u001b[39;49mskip_bound_arg,\n\u001b[1;32m   2518\u001b[0m                                     \u001b[39mglobals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mglobals\u001b[39;49m, \u001b[39mlocals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mlocals\u001b[39;49m, eval_str\u001b[39m=\u001b[39;49meval_str)\n\u001b[1;32m   2520\u001b[0m \u001b[39mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[1;32m   2521\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2522\u001b[0m                                    skip_bound_arg\u001b[39m=\u001b[39mskip_bound_arg)\n",
            "File \u001b[0;32m/usr/lib64/python3.11/inspect.py:2377\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2375\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m positional[:non_default_count]:\n\u001b[1;32m   2376\u001b[0m     kind \u001b[39m=\u001b[39m _POSITIONAL_ONLY \u001b[39mif\u001b[39;00m posonly_left \u001b[39melse\u001b[39;00m _POSITIONAL_OR_KEYWORD\n\u001b[0;32m-> 2377\u001b[0m     annotation \u001b[39m=\u001b[39m annotations\u001b[39m.\u001b[39mget(name, _empty)\n\u001b[1;32m   2378\u001b[0m     parameters\u001b[39m.\u001b[39mappend(Parameter(name, annotation\u001b[39m=\u001b[39mannotation,\n\u001b[1;32m   2379\u001b[0m                                 kind\u001b[39m=\u001b[39mkind))\n\u001b[1;32m   2380\u001b[0m     \u001b[39mif\u001b[39;00m posonly_left:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "FPS = 40\n",
        "\n",
        "try:\n",
        "    # Takes about 4 min to train\n",
        "    for ep in range(total_episodes):\n",
        "\n",
        "        prev_state, info = env.reset()\n",
        "        episodic_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # Don't start slowing down until we've gotten past the first bit\n",
        "            # if len(ep_reward_list) < 0 or len(ep_reward_list) > 75:\n",
        "                # Slow it down so we mere humans can see it\n",
        "                # time.sleep(1/FPS)\n",
        "            # Show the enviorment as it's updated\n",
        "            # env.render()\n",
        "\n",
        "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "            action = policy(tf_prev_state, noise)\n",
        "            # print(round(action[0]), 2)\n",
        "            # Recieve state and reward from environment.\n",
        "            state, reward, done, _, info = env.step(action)\n",
        "            # reward -= 100\n",
        "            # reward *= -1\n",
        "\n",
        "            buffer.record((prev_state, action, reward, state))\n",
        "            episodic_reward += reward\n",
        "\n",
        "            buffer.learn()\n",
        "            update_target(target_actor.variables, actor_model.variables, tau)\n",
        "            update_target(target_critic.variables, critic_model.variables, tau)\n",
        "\n",
        "            # End this episode when `done` is True\n",
        "            if done or len(ep_reward_list) > 300:\n",
        "                break\n",
        "\n",
        "            prev_state = state\n",
        "\n",
        "        ep_reward_list.append(episodic_reward)\n",
        "\n",
        "        # Mean of last 40 episodes\n",
        "        avg_reward = np.mean(ep_reward_list[-40:])\n",
        "        print(f\"Episode {ep}: Avg Reward is \\t{avg_reward}\")\n",
        "        avg_reward_list.append(avg_reward)\n",
        "\n",
        "    # Plotting graph\n",
        "    # Episodes versus Avg. Rewards\n",
        "    plt.plot(avg_reward_list)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "    plt.show()\n",
        "# Make sure the window closes if we finish or if there's an error\n",
        "finally:\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ2kaWXmJxlx"
      },
      "source": [
        "If training proceeds correctly, the average episodic reward will increase with time.\n",
        "\n",
        "Feel free to try different learning rates, `tau` values, and architectures for the\n",
        "Actor and Critic networks.\n",
        "\n",
        "The Inverted Pendulum problem has low complexity, but DDPG work great on many other\n",
        "problems.\n",
        "\n",
        "Another great environment to try this on is `LunarLandingContinuous-v2`, but it will take\n",
        "more episodes to obtain good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPcsNHBJJxlx"
      },
      "outputs": [],
      "source": [
        "# Save the weights\n",
        "actor_model.save_weights(\"pendulum_actor.h5\")\n",
        "critic_model.save_weights(\"pendulum_critic.h5\")\n",
        "\n",
        "target_actor.save_weights(\"pendulum_target_actor.h5\")\n",
        "target_critic.save_weights(\"pendulum_target_critic.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
